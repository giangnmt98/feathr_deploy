{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a89791bc-cfc2-4105-a541-a3392af3c314",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feathr Feature Store For Customer360 on Azure - Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4072c36-b190-4c8a-af43-dc004854aea4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook illustrates the use of Feathr Feature Store to create one of the use case for Customer 360. This usecase predicts Sales amount by the Discount offered. It includes following steps:\n",
    " \n",
    "1. Install and set up Feathr with Azure\n",
    "2. Create shareable features with Feathr feature definition configs.\n",
    "3. Create a training dataset via point-in-time feature join.\n",
    "4. Compute and write features.\n",
    "5. Train a model using these features to predict Sales Amount.\n",
    "6. Materialize feature value to online store.\n",
    "7. Fetch feature value in real-time from online store for online scoring.\n",
    "\n",
    "\n",
    "The feature flow is as follows:\n",
    "![Feature Engineering](./Feature_engineering_c360.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1632aaa6-35de-4d7f-9f88-ecfb1f927bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prerequisite: Provision cloud resources\n",
    "\n",
    "First step is to provision required cloud resources if you want to use Feathr. Feathr provides a python based client to interact with cloud resources.\n",
    "\n",
    "Please follow the steps [here](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to provision required cloud resources. Due to the complexity of the possible cloud environment, it is almost impossible to create a script that works for all the use cases. Because of this, [azure_resource_provision.sh](https://github.com/feathr-ai/feathr/blob/main/docs/how-to-guides/azure_resource_provision.sh) is a full end to end command line to create all the required resources, and you can tailor the script as needed, while [the companion documentation](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-cli.html) can be used as a complete guide for using that shell script.\n",
    "\n",
    "\n",
    "And the architecture is as below:\n",
    "\n",
    "![Architecture](https://github.com/feathr-ai/feathr/blob/main/docs/images/architecture.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80223a8e-8901-421c-b63d-4e11a6da5d88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sample Dataset\n",
    "\n",
    "In this demo, we use Feathr Feature Store to showcase Customer360 Features using Feathr. The dataset can be mounted onto a azure blob storage account and seen by executing the following command. The dataset is present in the current directory and it is referenced from [here](https://community.tableau.com/s/question/0D54T00000CWeX8SAL/sample-superstore-sales-excelxls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d38f6dc4-51f7-44cd-a82d-cd08e08260e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "key = \"blobstorekey\"\n",
    "acnt = \"studiofeathrazuredevsto\"\n",
    "container = \"studio-feathrazure-dev-fs\"\n",
    "mntpnt = \"/mnt/studio-feathrazure-dev-fs\"\n",
    "\n",
    "def mountStorageContainer(storageAccount, storageAccountKey, storageContainer, blobMountPoint):\n",
    "    try:\n",
    "        print(\"Mounting {0} to {1}:\".format(storageContainer, blobMountPoint))\n",
    "        dbutils.fs.unmount(blobMountPoint)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"....Container is not mounted; Attempting mounting now..\")\n",
    "        \n",
    "    mountStatus = dbutils.fs.mount(source = \"wasbs://{0}@{1}.blob.core.windows.net/\".format(storageContainer, storageAccount),\n",
    "                  mount_point = blobMountPoint,\n",
    "                  extra_configs = {\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storageAccount): storageAccountKey})\n",
    "    \n",
    "    print(\"....Status of mount is: \" + str(mountStatus))\n",
    "    print()\n",
    "\n",
    "    \n",
    "# mountStorageContainer(acnt,key,container,mntpnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/jovyan/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from feathr.utils.platform import is_databricks, is_jupyter\n",
    "from pyspark.sql import DataFrame, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/jovyan/work/customer360.csv\")\n",
    "df[\"sales_order_dt\"] = pd.to_datetime(df[\"sales_order_dt\"], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df[\"insert_dt\"] = pd.to_datetime(df[\"insert_dt\"], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df[\"last_modified_dt\"] = pd.to_datetime(df[\"last_modified_dt\"], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df[\"sales_launch_date\"] = pd.to_datetime(df[\"sales_launch_date\"], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df.to_parquet(\"customer360.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cms_txn_sk</th>\n",
       "      <th>sales_cust_id</th>\n",
       "      <th>sales_tran_id</th>\n",
       "      <th>sales_order_id</th>\n",
       "      <th>sales_item_quantity</th>\n",
       "      <th>sales_order_dt</th>\n",
       "      <th>cms_store_sk</th>\n",
       "      <th>sales_store_id</th>\n",
       "      <th>sales_store_name</th>\n",
       "      <th>sales_channel</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_launch_date</th>\n",
       "      <th>premium_prd</th>\n",
       "      <th>ship_mode</th>\n",
       "      <th>payment_preference</th>\n",
       "      <th>insert_dt</th>\n",
       "      <th>last_modified_dt</th>\n",
       "      <th>insert_by</th>\n",
       "      <th>last_modified_by</th>\n",
       "      <th>job_id</th>\n",
       "      <th>batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>JF-15295</td>\n",
       "      <td>txn4679</td>\n",
       "      <td>CA-2015-114510</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-11-23</td>\n",
       "      <td>N</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Debit card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GB-14575</td>\n",
       "      <td>txn4784</td>\n",
       "      <td>US-2018-147984</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>N</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>Digital Mode</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MC-17275</td>\n",
       "      <td>txn4850</td>\n",
       "      <td>CA-2015-107818</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>N</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Debit card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AS-10240</td>\n",
       "      <td>txn6215</td>\n",
       "      <td>CA-2015-128538</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>N</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Cash</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FH-14275</td>\n",
       "      <td>txn7572</td>\n",
       "      <td>CA-2015-100916</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Online</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-11-03</td>\n",
       "      <td>N</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Digital Mode</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9187</th>\n",
       "      <td>9188</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>txn9188</td>\n",
       "      <td>US-2016-130512</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>179.0</td>\n",
       "      <td>Store_284</td>\n",
       "      <td>Store284</td>\n",
       "      <td>Curbside</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>N</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9188</th>\n",
       "      <td>9189</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>txn9189</td>\n",
       "      <td>US-2016-130512</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>179.0</td>\n",
       "      <td>Store_284</td>\n",
       "      <td>Store284</td>\n",
       "      <td>Curbside</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>N</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>Cash</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9189</th>\n",
       "      <td>9190</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>txn9190</td>\n",
       "      <td>US-2016-130512</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>179.0</td>\n",
       "      <td>Store_284</td>\n",
       "      <td>Store284</td>\n",
       "      <td>Curbside</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Y</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9190</th>\n",
       "      <td>9191</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>txn9191</td>\n",
       "      <td>US-2016-130512</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>179.0</td>\n",
       "      <td>Store_284</td>\n",
       "      <td>Store284</td>\n",
       "      <td>Curbside</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>N</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>9192</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>txn9192</td>\n",
       "      <td>US-2016-130512</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>179.0</td>\n",
       "      <td>Store_284</td>\n",
       "      <td>Store284</td>\n",
       "      <td>Curbside</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>N</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>2022-03-16</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>Manual_Trigger</td>\n",
       "      <td>1234</td>\n",
       "      <td>15678-abcd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9192 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cms_txn_sk sales_cust_id sales_tran_id  sales_order_id  \\\n",
       "0              1      JF-15295       txn4679  CA-2015-114510   \n",
       "1              2      GB-14575       txn4784  US-2018-147984   \n",
       "2              3      MC-17275       txn4850  CA-2015-107818   \n",
       "3              4      AS-10240       txn6215  CA-2015-128538   \n",
       "4              5      FH-14275       txn7572  CA-2015-100916   \n",
       "...          ...           ...           ...             ...   \n",
       "9187        9188      SM-20320       txn9188  US-2016-130512   \n",
       "9188        9189      SM-20320       txn9189  US-2016-130512   \n",
       "9189        9190      SM-20320       txn9190  US-2016-130512   \n",
       "9190        9191      SM-20320       txn9191  US-2016-130512   \n",
       "9191        9192      SM-20320       txn9192  US-2016-130512   \n",
       "\n",
       "      sales_item_quantity sales_order_dt  cms_store_sk sales_store_id  \\\n",
       "0                       3     2020-01-27           NaN              0   \n",
       "1                       3     2020-01-18           NaN              0   \n",
       "2                       5     2020-01-13           NaN              0   \n",
       "3                       2     2019-12-14           NaN              0   \n",
       "4                       2     2019-11-16           NaN              0   \n",
       "...                   ...            ...           ...            ...   \n",
       "9187                    9     2019-10-01         179.0      Store_284   \n",
       "9188                    1     2019-10-01         179.0      Store_284   \n",
       "9189                    7     2019-10-01         179.0      Store_284   \n",
       "9190                   10     2019-10-01         179.0      Store_284   \n",
       "9191                    1     2019-10-01         179.0      Store_284   \n",
       "\n",
       "     sales_store_name sales_channel  ... sales_launch_date  premium_prd  \\\n",
       "0                None      Delivery  ...        2019-11-23            N   \n",
       "1                None      Delivery  ...        2019-11-16            N   \n",
       "2                None      Delivery  ...        2019-11-16            N   \n",
       "3                None      Delivery  ...        2019-10-01            N   \n",
       "4                None        Online  ...        2019-11-03            N   \n",
       "...               ...           ...  ...               ...          ...   \n",
       "9187         Store284      Curbside  ...        2019-10-01            N   \n",
       "9188         Store284      Curbside  ...        2019-10-01            N   \n",
       "9189         Store284      Curbside  ...        2019-10-01            Y   \n",
       "9190         Store284      Curbside  ...        2019-10-01            N   \n",
       "9191         Store284      Curbside  ...        2019-10-01            N   \n",
       "\n",
       "           ship_mode payment_preference   insert_dt  last_modified_dt  \\\n",
       "0        First Class         Debit card  2022-03-16        2022-03-16   \n",
       "1           Same Day       Digital Mode  2022-03-16        2022-03-16   \n",
       "2        First Class         Debit card  2022-03-16        2022-03-16   \n",
       "3        First Class               Cash  2022-03-16        2022-03-16   \n",
       "4        First Class       Digital Mode  2022-03-16        2022-03-16   \n",
       "...              ...                ...         ...               ...   \n",
       "9187  Standard Class        Credit Card  2022-03-16        2022-03-16   \n",
       "9188    Second Class               Cash  2022-03-16        2022-03-16   \n",
       "9189  Standard Class        Credit Card  2022-03-16        2022-03-16   \n",
       "9190  Standard Class        Credit Card  2022-03-16        2022-03-16   \n",
       "9191        Same Day        Credit Card  2022-03-16        2022-03-16   \n",
       "\n",
       "           insert_by  last_modified_by  job_id    batch_id  \n",
       "0     Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "1     Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "2     Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "3     Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "4     Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "...              ...               ...     ...         ...  \n",
       "9187  Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "9188  Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "9189  Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "9190  Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "9191  Manual_Trigger    Manual_Trigger    1234  15678-abcd  \n",
       "\n",
       "[9192 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(\"customer360.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"/home/jovyan/work/customer360.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import feathr\n",
    "import os\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    # Feature data types\n",
    "    BOOLEAN, FLOAT, INT32, ValueType, STRING,\n",
    "    # Feature data sources\n",
    "    INPUT_CONTEXT, HdfsSource,\n",
    "    # Feature aggregations\n",
    "    TypedKey, WindowAggTransformation,\n",
    "    # Feature types and anchor\n",
    "    DerivedFeature, Feature, FeatureAnchor,\n",
    "    # Materialization\n",
    "    BackfillTime, MaterializationSettings, RedisSink,\n",
    "    # Offline feature computation\n",
    "    FeatureQuery, ObservationSettings,\n",
    ")\n",
    "from feathr.datasets import nyc_taxi\n",
    "from feathr.spark_provider.feathr_configurations import SparkExecutionConfiguration\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from feathr.utils.platform import is_databricks, is_jupyter\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "Found jar file at ./feathr_2.12-1.0.5-rc4.jar\n",
      "\n",
      "api_version: 1\n",
      "project_config:\n",
      "  project_name: customer360\n",
      "  \n",
      "spark_config:\n",
      "  # choice for spark runtime. Currently support: azure_synapse, databricks, local\n",
      "  spark_cluster: 'local'\n",
      "  spark_result_output_parts: '1'\n",
      "  local:\n",
      "    master: 'local[*]'\n",
      "    spark.sql.shuffle.partitions: '10'\n",
      "    spark.driver.memory: \"10g\"\n",
      "    spark.executor.memory: \"10g\"\n",
      "    feathr_runtime_location: \"./feathr_2.12-1.0.5-rc4.jar\"\n",
      "\n",
      "online_store:\n",
      "  redis:\n",
      "    # Redis configs to access Redis cluster\n",
      "    host: '127.0.0.1'\n",
      "    port: 6379\n",
      "    ssl_enabled: False\n",
      "\n",
      "feature_registry:\n",
      "  # The API endpoint of the registry service\n",
      "  api_endpoint: \"http://127.0.0.1:8000/api/v1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(feathr.__version__)\n",
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"foobared\"  # default password for Redis\n",
    "\n",
    "\n",
    "# Make sure we get the Feathr jar name, assuming we just have one jar file.\n",
    "from pathlib import Path\n",
    "import glob\n",
    "jar_name = glob.glob(\"./*.jar\")[0]\n",
    "print(f\"Found jar file at {jar_name}\")\n",
    "\n",
    "PROJECT_NAME = \"customer360\"\n",
    "yaml_config = f\"\"\"\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: {PROJECT_NAME}\n",
    "  \n",
    "spark_config:\n",
    "  # choice for spark runtime. Currently support: azure_synapse, databricks, local\n",
    "  spark_cluster: 'local'\n",
    "  spark_result_output_parts: '1'\n",
    "  local:\n",
    "    master: 'local[*]'\n",
    "    spark.sql.shuffle.partitions: '10'\n",
    "    spark.driver.memory: \"10g\"\n",
    "    spark.executor.memory: \"10g\"\n",
    "    feathr_runtime_location: \"{jar_name}\"\n",
    "\n",
    "online_store:\n",
    "  redis:\n",
    "    # Redis configs to access Redis cluster\n",
    "    host: '127.0.0.1'\n",
    "    port: 6379\n",
    "    ssl_enabled: False\n",
    "\n",
    "feature_registry:\n",
    "  # The API endpoint of the registry service\n",
    "  api_endpoint: \"http://127.0.0.1:8000/api/v1\"\n",
    "\"\"\"\n",
    "feathr_workspace_folder = Path(\"./360_feathr_config.yaml\")\n",
    "feathr_workspace_folder.parent.mkdir(exist_ok=True, parents=True)\n",
    "feathr_workspace_folder.write_text(yaml_config)\n",
    "print(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32261356-9c9e-4988-9754-ad6fc1c447e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prerequisite: Install Feathr\n",
    "\n",
    "Install Feathr using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d87942f-db42-48cd-bf8f-f79c3214ce92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Prerequisite: Configure the required environment\n",
    "\n",
    "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
    "\n",
    "The code below will write this configuration string to a temporary location and load it to Feathr. Please still refer to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bda4f77-8418-460f-83ad-bb442f9a0525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setup necessary environment variables\n",
    "\n",
    "You have to setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec09a17d-ec64-4b9f-999f-9a71a508eaed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Initialize a feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88aec1c1-2bdc-42d2-918d-48c0e28fdd0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:26:55.923 | INFO     | feathr.utils._env_config_reader:get:62 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.925 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.926 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.926 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.927 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.928 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "No offline storage enabled.\n",
      "2024-08-06 07:26:55.929 | INFO     | feathr.utils._env_config_reader:get:62 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.930 | INFO     | feathr.utils._env_config_reader:get:62 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:26:55.931 | INFO     | feathr.client:__init__:208 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the notebook is runnong on Jupyter, start a spark session:\n",
    "if is_jupyter():\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"feathr\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0,io.delta:delta-core_2.12:2.1.1\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config(\"spark.ui.port\", \"8080\")  # Set ui port other than the default one (4040) so that feathr spark job doesn't fail. \n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "# Else, you must already have a spark session object available in databricks or synapse notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64f272f0-7008-4de7-89fe-a9d32f5573a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Sources Section\n",
    "A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. See the python documentation to get the details on each input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path = data_file_path \n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\"\n",
    "TIMESTAMP_COL = \"sales_order_dt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6162dfd7-0791-4e9b-8200-da7710272c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def preprocessing(df: DataFrame) -> DataFrame:\n",
    "#     import pyspark.sql.functions as F\n",
    "#     df = df.filter(F.col(\"ship_mode\")==\"First Class\")\n",
    "#     return df\n",
    "\n",
    "batch_source = HdfsSource(name=\"cosmos_final_data\",\n",
    "                          path=data_source_path,\n",
    "                          event_timestamp_column=TIMESTAMP_COL,\n",
    "                          # preprocessing=preprocessing,\n",
    "                          timestamp_format=TIMESTAMP_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d08913f-e416-46e3-9bf3-31f50e41139f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Defining Features with Feathr:\n",
    "In Feathr, a feature is viewed as a function, mapping from entity id or key, and timestamp to a feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa303679-7be2-430b-8194-19c90a28c4af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Anchors and  Features\n",
    "A feature is called an anchored feature when the feature is directly extracted from the source data, rather than computed on top of other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d95d006-8d9a-4e63-b7b0-2c88a166c6cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f_sales_cust_id = Feature(name = \"f_sales_cust_id\",\n",
    "                          feature_type = STRING, transform = \"sales_cust_id\" )\n",
    "\n",
    "f_sales_tran_id = Feature(name = \"f_sales_tran_id\",\n",
    "                          feature_type = STRING, transform = \"sales_tran_id\" )\n",
    "\n",
    "f_sales_order_id = Feature(name = \"f_sales_order_id\",\n",
    "                           feature_type = STRING, transform = \"sales_order_id\" )\n",
    "\n",
    "f_sales_item_quantity = Feature(name = \"f_sales_item_quantity\", \n",
    "                                feature_type = INT32, transform = \"cast_float(sales_item_quantity)\" )\n",
    "\n",
    "f_sales_order_dt = Feature(name = \"f_sales_order_dt\",\n",
    "                           feature_type = STRING, transform = \"sales_order_dt\" )\n",
    "\n",
    "f_sales_sell_price = Feature(name = \"f_sales_sell_price\",\n",
    "                             feature_type = INT32, transform = \"cast_float(sales_sell_price)\" )\n",
    "\n",
    "f_sales_discount_amt = Feature(name = \"f_sales_discount_amt\",\n",
    "                               feature_type = INT32, transform = \"cast_float(sales_discount_amt)\" )\n",
    "\n",
    "f_payment_preference = Feature(name = \"f_payment_preference\",\n",
    "                               feature_type = STRING, transform = \"payment_preference\" )\n",
    "\n",
    "\n",
    "features = [f_sales_cust_id, f_sales_tran_id, f_sales_order_id, f_sales_item_quantity, \n",
    "            f_sales_order_dt, f_sales_sell_price, f_sales_discount_amt, f_payment_preference]\n",
    "\n",
    "request_anchor = FeatureAnchor(name=\"request_features\",\n",
    "                                source=INPUT_CONTEXT,\n",
    "                                features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b84cebbe-884f-4665-9df7-dc3a16037fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Derived Features\n",
    "Derived features are the features that are computed from other features. They could be computed from anchored features, or other derived features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d620ae76-b8ed-4bfe-a0dd-2e50ffd79212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f_total_sales_amount = DerivedFeature(name = \"f_total_sales_amount\",\n",
    "                                   feature_type = FLOAT,\n",
    "                                   input_features = [f_sales_item_quantity,f_sales_sell_price],\n",
    "                                   transform = \"f_sales_item_quantity * f_sales_sell_price\")\n",
    "\n",
    "f_total_sales_discount= DerivedFeature(name = \"f_total_sales_discount\",\n",
    "                                   feature_type = FLOAT,\n",
    "                                   input_features = [f_sales_item_quantity,f_sales_discount_amt],\n",
    "                                   transform = \"f_sales_item_quantity * f_sales_discount_amt\")\n",
    "\n",
    "\n",
    "f_total_amount_paid= DerivedFeature(name = \"f_total_amount_paid\",\n",
    "                                   feature_type = FLOAT,\n",
    "                                   input_features = [f_sales_sell_price,f_sales_discount_amt],\n",
    "                                   transform =\"f_sales_sell_price - f_sales_discount_amt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3230211b-c978-44d3-9996-edd53fa952f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Aggregate features and anchor the features to batch source.\n",
    "\n",
    "Note that if the data source is from the observation data, the source section should be INPUT_CONTEXT to indicate the source of those defined anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef204054-6638-4ff5-ba46-330256f553ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_ID = TypedKey(key_column=\"sales_cust_id\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"customer ID\",\n",
    "                       full_name=\"cosmos.sales_cust_id\")\n",
    "\n",
    "agg_features = [Feature(name=\"f_avg_customer_sales_amount\",\n",
    "                        key=customer_ID,\n",
    "                        feature_type=FLOAT,\n",
    "                        transform=WindowAggTransformation(agg_expr=\"cast_float(sales_sell_price)\",\n",
    "                                                          agg_func=\"AVG\",\n",
    "                                                          window=\"1d\")),\n",
    "               \n",
    "               Feature(name=\"f_avg_customer_discount_amount\",\n",
    "                        key=customer_ID,\n",
    "                        feature_type=FLOAT,\n",
    "                        transform=WindowAggTransformation(agg_expr=\"cast_float(sales_discount_amt)\",\n",
    "                                                          agg_func=\"AVG\",\n",
    "                                                          window=\"1d\")),\n",
    "               \n",
    "              Feature(name=\"f_avg_item_ordered_by_customer\",\n",
    "                        key=customer_ID,\n",
    "                        feature_type=FLOAT,\n",
    "                        transform=WindowAggTransformation(agg_expr=\"cast_float(sales_item_quantity)\",\n",
    "                                                          agg_func=\"AVG\",\n",
    "                                                          window=\"1d\"))]\n",
    "\n",
    "agg_anchor = FeatureAnchor(name=\"aggregationFeatures\",\n",
    "                           source=batch_source,\n",
    "                           features=agg_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e803a63-80b6-40ad-9419-422ca1db3d97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Building Features\n",
    "And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "949a79bd-e4e9-487a-9a5c-b04cdecba3b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.build_features(anchor_list=[request_anchor,agg_anchor],\n",
    "                      derived_feature_list=[f_total_sales_amount, f_total_sales_discount,f_total_amount_paid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "684aef42-53e1-4548-b604-9a581abda253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Registering Features\n",
    "We can also register the features with an Apache Atlas compatible service, such as Azure Purview, and share the registered features across teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c486ea65-9ee9-4f73-aa61-33873ade8fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#client.register_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00f5b4d9-5054-4a53-a511-2b8380c08ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#client.list_registered_features(project_name=\"customer360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30e09585-3917-4d3b-8681-15360ad74972",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create training data using point-in-time correct feature join\n",
    "A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column.\n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify what features and how these features should be joined to the observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                  \"f_avg_item_ordered_by_customer\", \n",
    "                  \"f_avg_customer_discount_amount\", \n",
    "                  \"f_avg_customer_sales_amount\", \n",
    "                  \"f_total_sales_discount\",\n",
    "                  \"f_total_sales_amount\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c2baf2c-835f-4aa9-8e01-b7c0e8711081",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_query = FeatureQuery(\n",
    "    feature_list=feature_names, key=customer_ID)\n",
    "settings = ObservationSettings(\n",
    "    observation_path=data_source_path,\n",
    "    event_timestamp_column=TIMESTAMP_COL,\n",
    "    timestamp_format=TIMESTAMP_FORMAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cf3eb6a-d014-429f-9f57-28aa2870785d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Materialize feature value into offline storage\n",
    "While Feathr can compute the feature value from the feature definition on-the-fly at request time, it can also pre-compute and materialize the feature value to offline and/or online storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘customer360’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir {PROJECT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FORMAT=\"parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:26:59.615 | ERROR    | feathr.utils.job_utils:get_result_df:170 - Failed to load result files from /home/jovyan/work/customer360.parquet with format csv.\n",
      "2024-08-06 07:26:59.616 | WARNING  | feathr.utils.job_utils:get_cloud_file_column_names:248 - failed to load cloud files from the path: /home/jovyan/work/customer360.parquet because of lack of permission or invalid path.\n",
      "Feathr is unable to read the Observation data from /home/jovyan/work/customer360.parquet due to permission issue or invalid path. Please either grant the permission or supply the observation column names in the filed: observation_column_names.\n",
      "2024-08-06 07:26:59.707 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:26:59.707 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_join_job20240806072659\n",
      "2024-08-06 07:26:59.708 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:26:59.719 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_join_job20240806072659/log.\n",
      "2024-08-06 07:26:59.720 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 36397.\n",
      "2024-08-06 07:26:59.722 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:163 - 1 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-08-06 07:26:59.723 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:164 - Please check auto generated spark command in debug/customer360_feathr_feature_join_job20240806072659/command.sh and detail logs in debug/customer360_feathr_feature_join_job20240806072659/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:27:30.749 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:229 - Spark job with pid 36397 finished in: 31 seconds                     with returncode 0\n"
     ]
    }
   ],
   "source": [
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=feature_query,\n",
    "    # For more details, see https://feathr-ai.github.io/feathr/how-to-guides/feathr-job-configuration.html\n",
    "    execution_configurations=SparkExecutionConfiguration({\n",
    "        \"spark.feathr.outputFormat\": DATA_FORMAT,\n",
    "    }),\n",
    "    output_path=PROJECT_NAME + f\"/features.{DATA_FORMAT}\",\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b54f11fe-0a2e-4223-9090-68b12d3b3fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading training data from offline storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:27:30.771 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:163 - 1 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-08-06 07:27:30.772 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:164 - Please check auto generated spark command in debug/customer360_feathr_feature_join_job20240806072659/command.sh and detail logs in debug/customer360_feathr_feature_join_job20240806072659/log.\n",
      "2024-08-06 07:27:30.772 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:229 - Spark job with pid 36397 finished in: 0 seconds                     with returncode 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+---------------------------+----------------------+--------------------+\n",
      "|f_avg_item_ordered_by_customer|f_avg_customer_discount_amount|f_avg_customer_sales_amount|f_total_sales_discount|f_total_sales_amount|\n",
      "+------------------------------+------------------------------+---------------------------+----------------------+--------------------+\n",
      "|                           6.0|                     18.429613|                   80.64266|                  48.0|               312.0|\n",
      "|                     3.3333333|                      2.501458|                  37.373333|                   0.0|                 8.0|\n",
      "|                           6.0|                     14.753651|                   198.6435|                   2.0|                56.0|\n",
      "|                           5.0|                     2.4403865|                      29.16|                  10.0|               145.0|\n",
      "|                           8.5|                     6.9802566|                     53.915|                  10.0|                70.0|\n",
      "+------------------------------+------------------------------+---------------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show feature results\n",
    "df = get_result_df(\n",
    "    spark=spark,\n",
    "    client=client,\n",
    "    data_format=DATA_FORMAT,\n",
    ")\n",
    "df.select(feature_names).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e398ee6-e2eb-4cf6-90b1-a71dc693a2c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Train a ML model\n",
    "\n",
    "After getting all the features, let's train a machine learning model with the converted feature by Feathr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6948c79-0b06-41a7-8df0-4332d40a5b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MAPE:\n",
      "0.2845262685819667\n",
      "\n",
      "Model Accuracy:\n",
      "0.7154737314180333\n"
     ]
    }
   ],
   "source": [
    "X = pandas_df['f_total_sales_discount']\n",
    "y = pandas_df['f_total_sales_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n",
    "\n",
    "# Add a constant to get an intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the resgression line using 'OLS'\n",
    "lr = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Add a constant to X_test\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "# Predict the y values corresponding to X_test_sm\n",
    "y_pred = lr.predict(X_test_sm)\n",
    "\n",
    "# Checking the R-squared on the test set\n",
    "\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "r_squared\n",
    "\n",
    "\n",
    "print(\"Model MAPE:\")\n",
    "print(1 - r_squared)\n",
    "print()\n",
    "print(\"Model Accuracy:\")\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69cb5413-7327-4e64-81d3-f76010a6af52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Materialize feature value into online storage\n",
    "We can push the generated features to the online store like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘materialize_hdfs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir materialize_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:27:35.148 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.149 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.150 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.150 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.151 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:27:35.151 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806072735\n",
      "2024-08-06 07:27:35.152 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:27:35.164 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806072735/log.\n",
      "2024-08-06 07:27:35.165 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 38081.\n",
      "2024-08-06 07:27:35.204 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.205 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.205 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.206 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.206 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:27:35.207 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806072735\n",
      "2024-08-06 07:27:35.208 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:27:35.225 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806072735/log.\n",
      "2024-08-06 07:27:35.226 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 38104.\n",
      "2024-08-06 07:27:35.265 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.266 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.267 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:27:35.268 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:27:35.268 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:27:35.269 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806072735\n",
      "2024-08-06 07:27:35.269 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:27:35.283 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806072735/log.\n",
      "2024-08-06 07:27:35.284 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 38128.\n",
      "2024-08-06 07:27:35.285 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:163 - 4 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-08-06 07:27:35.286 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:164 - Please check auto generated spark command in debug/customer360_feathr_feature_materialization_job20240806072735/command.sh and detail logs in debug/customer360_feathr_feature_materialization_job20240806072735/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:28:06.317 | WARNING  | feathr.spark_provider._localspark_submission:wait_for_completion:201 - Spark job with pid 38128 not completed after 20 sec                     time out setting. Spark Logs:\n",
      "2024-08-06 07:28:06.318 | ERROR    | feathr.spark_provider._localspark_submission:wait_for_completion:207 - Source location is: SimplePath(path=/home/jovyan/work/customer360.parquet)\n",
      "generateFeaturesAsDF: 1. call analyze features, e.g. group features\n",
      "generateFeaturesAsDF: 2. Get AnchorDFMap for Anchored features\n",
      "generateFeaturesAsDF: 3. Load user specified default values and feature types, if any.\n",
      "generateFeaturesAsDF: 4. Calculate anchored features\n",
      "NonTimeBasedDataSourceAccessor loading source SimplePath(path=/home/jovyan/work/customer360.parquet)\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "generateFeaturesAsDF: 5. Group features based on grouping specified in output processors\n",
      "generateFeaturesAsDF: 6. Substitute defaults at this stage since all anchored features are generated and grouped together. Substitute before generating derived features.\n",
      "generateFeaturesAsDF: 7. Calculate derived features.\n",
      "generateFeaturesAsDF: 8. Prune feature columns before handing it off to output processors. As part of the pruning columns are renamed to a standard and unwanted columns, features are dropped.\n",
      "generateFeaturesAsDF: 9. apply outputProcessors\n",
      "\n",
      "2024-08-06 07:28:06.319 | WARNING  | feathr.spark_provider._localspark_submission:_clean_up:236 - Terminate the spark job due to as clean_up is set to True.\n"
     ]
    }
   ],
   "source": [
    "FEATURE_TABLE_NAME = \"Customer360\"\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from feathr import HdfsSink\n",
    "# Time range to materialize\n",
    "backfill_time = BackfillTime(start=datetime(2019, 12, 31), end=datetime(2020, 1, 2), step=timedelta(days=1))\n",
    "from feathr import HdfsSink\n",
    "# Destinations:\n",
    "# For online store,\n",
    "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "hdfs_sink = HdfsSink(output_path=\"materialize_hdfs\", store_name=\"df0\")\n",
    "# For offline store,\n",
    "# adls_sink = HdfsSink(output_path=)\n",
    "\n",
    "settings = MaterializationSettings(\n",
    "    name=FEATURE_TABLE_NAME + \".job\",  # job name\n",
    "    backfill_time=backfill_time,\n",
    "    sinks=[ redis_sink, hdfs_sink],  # or adls_sink\n",
    "    feature_names=[\"f_avg_item_ordered_by_customer\",\"f_avg_customer_discount_amount\"],\n",
    ")\n",
    "# \n",
    "client.materialize_features(\n",
    "    settings=settings,\n",
    "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\", \n",
    "                              \"spark.feathr.hdfs.local.enable\":\"true\",\n",
    "                              \"spark.sql.shuffle.partitions\": '1'},\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+--------+\n",
      "|f_avg_customer_discount_amount|f_avg_item_ordered_by_customer|    key0|\n",
      "+------------------------------+------------------------------+--------+\n",
      "|                     0.6421022|                           1.0|JM-15580|\n",
      "|                    0.69970566|                     5.3333335|AZ-10750|\n",
      "|                      2.516956|                     4.6666665|BK-11260|\n",
      "|                      7.981272|                           3.0|BM-11785|\n",
      "|                     1.4155095|                           7.5|HM-14860|\n",
      "|                     11.548739|                           9.0|KH-16690|\n",
      "|                     4.6880927|                           5.0|PN-18775|\n",
      "|                    0.04808883|                           6.0|KN-16390|\n",
      "|                     2.1529326|                           2.5|GM-14500|\n",
      "|                     23.128746|                           2.5|KN-16390|\n",
      "+------------------------------+------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from feathr import get_result_df\n",
    "path = \"materialize_hdfs/df0*/daily/2020/01/*\"\n",
    "df = get_result_df(spark=spark,client=client, format=\"parquet\", res_url=path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KN-16390': [2.5, 23.128746032714844], 'HM-14860': [7.5, 1.4155094623565674]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note, to get a single key, you may use client.get_online_features instead\n",
    "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "materialized_feature_values = client.multi_get_online_features(\n",
    "    feature_table=FEATURE_TABLE_NAME,\n",
    "    keys=[\"KN-16390\", \"HM-14860\", \"KN-16390\"],\n",
    "    feature_names=[\"f_avg_item_ordered_by_customer\",\"f_avg_customer_discount_amount\"],\n",
    ")\n",
    "materialized_feature_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "706dcf1b-64d1-47d0-8bbe-88c8af82a464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:50:18.929 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:18.930 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:18.930 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:18.930 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:18.931 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:18.931 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075018\n",
      "2024-08-06 07:50:18.932 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:18.944 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075018/log.\n",
      "2024-08-06 07:50:18.946 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50018.\n",
      "2024-08-06 07:50:19.005 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.007 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.008 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.008 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.009 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.009 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.011 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.030 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.033 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50042.\n",
      "2024-08-06 07:50:19.073 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.074 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.074 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.075 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.076 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.076 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.077 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.088 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.091 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50066.\n",
      "2024-08-06 07:50:19.134 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.134 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.135 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.135 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.136 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.136 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.137 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.149 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.150 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50089.\n",
      "2024-08-06 07:50:19.195 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.196 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.197 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.198 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.199 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.199 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.200 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.213 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.215 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50119.\n",
      "2024-08-06 07:50:19.265 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.266 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.268 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.269 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.270 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.271 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.272 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.291 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.294 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50157.\n",
      "2024-08-06 07:50:19.361 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.363 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.364 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.365 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.366 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.367 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.369 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.390 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.393 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50201.\n",
      "2024-08-06 07:50:19.459 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.461 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.462 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-06 07:50:19.463 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-06 07:50:19.464 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-06 07:50:19.466 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:288 - Spark log path is debug/customer360_feathr_feature_materialization_job20240806075019\n",
      "2024-08-06 07:50:19.469 | INFO     | feathr.spark_provider._localspark_submission:_init_args:263 - Spark job: customer360_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-06 07:50:19.495 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:143 - Detail job stdout and stderr are in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n",
      "2024-08-06 07:50:19.499 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:153 - Local Spark job submit with pid: 50247.\n",
      "2024-08-06 07:50:19.502 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:163 - 70 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-08-06 07:50:19.503 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:164 - Please check auto generated spark command in debug/customer360_feathr_feature_materialization_job20240806075019/command.sh and detail logs in debug/customer360_feathr_feature_materialization_job20240806075019/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 07:50:50.539 | WARNING  | feathr.spark_provider._localspark_submission:wait_for_completion:213 - Spark job with pid 50247 is not successful. Spark Logs:\n",
      "2024-08-06 07:50:50.542 | ERROR    | feathr.spark_provider._localspark_submission:wait_for_completion:216 - \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Spark job failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     18\u001b[0m client\u001b[38;5;241m.\u001b[39mmaterialize_features(\n\u001b[1;32m     19\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m     20\u001b[0m     execution_configurations\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.feathr.outputFormat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     21\u001b[0m                               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.feathr.hdfs.local.enable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_job_to_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_sec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/feathr_project/feathr/client.py:977\u001b[0m, in \u001b[0;36mFeathrClient.wait_job_to_finish\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark job failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Spark job failed."
     ]
    }
   ],
   "source": [
    "FEATURE_TABLE_NAME = \"FullCustomer360\"\n",
    "from feathr import HdfsSink\n",
    "# Destinations:\n",
    "# For online store,\n",
    "backfill_time = BackfillTime(start=datetime(2020, 2, 7), end=datetime(2020, 2, 14), step=timedelta(days=1))\n",
    "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "hdfs_sink = HdfsSink(output_path=\"materialize_hdfs\", store_name=\"df360\")\n",
    "# For offline store,\n",
    "# adls_sink = HdfsSink(output_path=)\n",
    "\n",
    "settings = MaterializationSettings(\n",
    "    name=FEATURE_TABLE_NAME + \".job\",  # job name\n",
    "    backfill_time=backfill_time,\n",
    "    sinks=[hdfs_sink, redis_sink],  # or adls_sink\n",
    "    feature_names=[\"f_avg_item_ordered_by_customer\",\"f_avg_customer_discount_amount\"],\n",
    ")\n",
    "# \n",
    "client.materialize_features(\n",
    "    settings=settings,\n",
    "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\", \n",
    "                              \"spark.feathr.hdfs.local.enable\":\"true\",\n",
    "                              \"spark.sql.shuffle.partitions\": '4'},\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Customer360_MS_V2",
   "notebookOrigID": 2897062443582288,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
