{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "384e5e16-7213-4186-9d04-09d03b155534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feathr Quick Start Notebook\n",
    "\n",
    "This notebook illustrates the use of Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "The major problems Feathr solves are:\n",
    "\n",
    "1. Create, share and manage useful features from raw source data.\n",
    "2. Provide Point-in-time feature join to create training dataset to ensure no data leakage.\n",
    "3. Deploy the same feature data to online store to eliminate training and inference data skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Feathr has native cloud integration. First step is to provision required cloud resources if you want to use Feathr.\n",
    "\n",
    "Follow the [Feathr ARM deployment guide](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to run Feathr on Azure. This allows you to quickly get started with automated deployment using Azure Resource Manager template. For more details, please refer [README.md](https://github.com/feathr-ai/feathr#%EF%B8%8F-running-feathr-on-cloud-with-a-few-simple-steps).\n",
    "\n",
    "Additionally, to run this notebook, you'll need to install `feathr` pip package. For local spark, simply run `pip install feathr` on the machine that runs this notebook. To use Databricks or Azure Synapse Analytics, please see dependency management documents:\n",
    "- [Azure Databricks dependency management](https://learn.microsoft.com/en-us/azure/databricks/libraries/)\n",
    "- [Azure Synapse Analytics dependency management](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Steps\n",
    "\n",
    "This tutorial demonstrates the key capabilities of Feathr, including:\n",
    "\n",
    "1. Install Feathr and necessary dependencies\n",
    "2. Create shareable features with Feathr feature definition configs\n",
    "3. Create training data using point-in-time correct feature join\n",
    "4. Train a prediction model and evaluate the model and features\n",
    "5. Register the features to share across teams\n",
    "6. Materialize feature values for online scoring\n",
    "\n",
    "The overall data flow is as follows:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/feathr-ai/feathr/main/docs/images/feature_flow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Feathr and Necessary Dependancies\n",
    "\n",
    "Install feathr and necessary packages by running one of following commends if you haven't installed them already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install feathr from the latest codes in the repo:\n",
    "#%pip install \"git+https://github.com/feathr-ai/feathr.git#subdirectory=feathr_project&egg=feathr[notebook]\" \n",
    "\n",
    "# To install the latest release:\n",
    "#%pip install \"feathr[notebook]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80223a02-631c-40c8-91b3-a037249ffff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import feathr\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    # Feature data types\n",
    "    BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    # Feature data sources\n",
    "    INPUT_CONTEXT, HdfsSource,\n",
    "    # Feature aggregations\n",
    "    TypedKey, WindowAggTransformation,\n",
    "    # Feature types and anchor\n",
    "    DerivedFeature, Feature, FeatureAnchor,\n",
    "    # Materialization\n",
    "    BackfillTime, MaterializationSettings, RedisSink,\n",
    "    # Offline feature computation\n",
    "    FeatureQuery, ObservationSettings,\n",
    ")\n",
    "from feathr.datasets import nyc_taxi\n",
    "from feathr.spark_provider.feathr_configurations import SparkExecutionConfiguration\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from feathr.utils.platform import is_databricks, is_jupyter\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Shareable Features with Feathr Feature Definition Configs\n",
    "\n",
    "First, we define all the necessary resource key values for authentication. These values are retrieved by using [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) cloud key value store. For authentication, we use Azure CLI credential in this notebook, but you may add secrets' list and get permission for the necessary service principal instead of running `az login --use-device-code`.\n",
    "\n",
    "Please refer to [A note on using azure key vault to store credentials](https://github.com/feathr-ai/feathr/blob/41e7496b38c43af6d7f8f1de842f657b27840f6d/docs/how-to-guides/feathr-configuration-and-env.md#a-note-on-using-azure-key-vault-to-store-credentials) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = None  # TODO fill the value used to deploy the resources via ARM template\n",
    "PROJECT_NAME = \"nyc_taxi\"\n",
    "\n",
    "# Currently support: 'azure_synapse', 'databricks', and 'local' \n",
    "SPARK_CLUSTER = \"local\"\n",
    "\n",
    "# TODO fill values to use databricks cluster:\n",
    "DATABRICKS_CLUSTER_ID = None             # Set Databricks cluster id to use an existing cluster\n",
    "if is_databricks():\n",
    "    # If this notebook is running on Databricks, its context can be used to retrieve token and instance URL\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = ctx.apiToken().get()\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = f\"https://{ctx.tags().get('browserHostName').get()}\"\n",
    "else:\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = None                  # Set Databricks workspace token to use databricks\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = None  # Set Databricks workspace url to use databricks\n",
    "\n",
    "# TODO fill values to use Azure Synapse cluster:\n",
    "AZURE_SYNAPSE_SPARK_POOL = None  # Set Azure Synapse Spark pool name\n",
    "AZURE_SYNAPSE_URL = None         # Set Azure Synapse workspace url to use Azure Synapse\n",
    "ADLS_KEY = None                  # Set Azure Data Lake Storage key to use Azure Synapse\n",
    "\n",
    "# An existing Feathr config file path. If None, we'll generate a new config based on the constants in this cell.\n",
    "FEATHR_CONFIG_PATH = None\n",
    "\n",
    "# If set True, use an interactive browser authentication to get the redis password.\n",
    "USE_CLI_AUTH = False\n",
    "\n",
    "# If set True, register the features to Feathr registry.\n",
    "REGISTER_FEATURES = False\n",
    "\n",
    "# (For the notebook test pipeline) If true, use ScrapBook package to collect the results.\n",
    "SCRAP_RESULTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Databricks as the feathr client's target platform, you may need to set a databricks token to an environment variable like:\n",
    "\n",
    "`export DATABRICKS_WORKSPACE_TOKEN_VALUE=your-token`\n",
    "\n",
    "or in the notebook cell,\n",
    "\n",
    "`os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = your-token`\n",
    "\n",
    "If you are running this notebook on Databricks, the token will be automatically retrieved by using the current Databricks notebook context.\n",
    "\n",
    "On the other hand, to use Azure Synapse cluster, you have to specify the synapse workspace storage key:\n",
    "\n",
    "`export ADLS_KEY=your-key`\n",
    "\n",
    "or in the notebook cell,\n",
    "\n",
    "`os.environ[\"ADLS_KEY\"] = your-key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_CLUSTER == \"azure_synapse\" and not os.environ.get(\"ADLS_KEY\"):\n",
    "    os.environ[\"ADLS_KEY\"] = ADLS_KEY\n",
    "elif SPARK_CLUSTER == \"databricks\" and not os.environ.get(\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"):\n",
    "    os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = DATABRICKS_WORKSPACE_TOKEN_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an authentication credential to access Azure resources and register features\n",
    "if USE_CLI_AUTH:\n",
    "    # Use AZ CLI interactive browser authentication\n",
    "    !az login --use-device-code\n",
    "    from azure.identity import AzureCliCredential\n",
    "    credential = AzureCliCredential(additionally_allowed_tenants=['*'],)\n",
    "elif \"AZURE_TENANT_ID\" in os.environ and \"AZURE_CLIENT_ID\" in os.environ and \"AZURE_CLIENT_SECRET\" in os.environ:\n",
    "    # Use Environment variable secret\n",
    "    from azure.identity import EnvironmentCredential\n",
    "    credential = EnvironmentCredential()\n",
    "else:\n",
    "    # Try to use the default credential\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    credential = DefaultAzureCredential(\n",
    "        exclude_interactive_browser_credential=False,\n",
    "        additionally_allowed_tenants=['*'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41d3648a-9bc9-40dc-90da-bc82b21ef9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configurations\n",
    "\n",
    "Feathr uses a yaml file to define configurations. Please refer to [feathr_config.yaml]( https://github.com//feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) for the meaning of each field.\n",
    "\n",
    "All the Feathr configurations can be set to the yaml file via keyword arguments of `generate_config` helper function. Each keyword argument should be the concatenation of different layers of the config name using `__` as a separator.\n",
    "For example, if you want to specify a different value for the feature registry api endpoint, you can pass `        feature_registry__api_endpoint=\"YOUR-API-ENDPOINT-URL\"`.\n",
    "\n",
    "Note, a default value for the api endpoint will be set based on `RESOURCE_PREFIX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JDBC_USER'] = \"root\"\n",
    "os.environ['JDBC_PASSWORD'] = \"DsteamIC2024\"\n",
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"foobared\"  # default password for Redis\n",
    "\n",
    "\n",
    "# Make sure we get the Feathr jar name, assuming we just have one jar file.\n",
    "PROJECT_NAME = \"nyc_taxi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the configurations can be overwritten by environment variables with concatenation of `__` for different layers of the config file, same as how you may pass the keyword arguments to `generate_config` utility function.\n",
    "\n",
    "For example, `feathr_runtime_location` for databricks config can be overwritten by setting `spark_config__databricks__feathr_runtime_location` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fef7f2f-df19-4f53-90a5-ff7999ed983d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9713a2df-c7b2-4562-88b0-b7acce3cc43a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:26:16.893 | INFO     | feathr.utils._env_config_reader:get:62 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.896 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.897 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.898 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.899 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.900 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "No offline storage enabled.\n",
      "2024-09-06 16:26:16.901 | INFO     | feathr.utils._env_config_reader:get:62 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.901 | INFO     | feathr.utils._env_config_reader:get:62 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:16.902 | INFO     | feathr.client:__init__:208 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "feathr_workspace_folder = Path(f\"./{PROJECT_NAME}_feathr_config.yaml\")\n",
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3b64bda-d42c-4a64-b976-0fb604cf38c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare the NYC taxi fare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the notebook is runnong on Jupyter, start a spark session:\n",
    "if is_jupyter():\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"feathr\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0,io.delta:delta-core_2.12:2.1.1\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config(\"spark.ui.port\", \"8080\")  # Set ui port other than the default one (4040) so that feathr spark job doesn't fail. \n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "# Else, you must already have a spark session object available in databricks or synapse notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dbfs if the notebook is running on Databricks\n",
    "if is_databricks():\n",
    "    WORKING_DIR = f\"/dbfs/{PROJECT_NAME}\"\n",
    "else:\n",
    "    WORKING_DIR = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4ccd7b3-298a-4e5a-8eec-b7e309db393e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|trip_id|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+-------+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|      0|     2.0| 2020-04-01 00:44:02|  2020-04-01 00:52:23|                 N|       1.0|          42|          41|            1.0|         1.68|        8.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         9.3|         1.0|      1.0|                 0.0|\n",
      "|      1|     2.0| 2020-04-01 00:24:39|  2020-04-01 00:33:06|                 N|       1.0|         244|         247|            2.0|         1.94|        9.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.3|         2.0|      1.0|                 0.0|\n",
      "|      2|     2.0| 2020-04-01 00:45:06|  2020-04-01 00:51:13|                 N|       1.0|         244|         243|            3.0|          1.0|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         2.0|      1.0|                 0.0|\n",
      "|      3|     2.0| 2020-04-01 00:45:06|  2020-04-01 01:04:39|                 N|       1.0|         244|         243|            2.0|         2.81|       12.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        13.3|         2.0|      1.0|                 0.0|\n",
      "|      4|     2.0| 2020-04-01 00:00:23|  2020-04-01 00:16:13|                 N|       1.0|          75|         169|            1.0|         6.79|       21.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        22.3|         1.0|      1.0|                 0.0|\n",
      "+-------+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the data file\n",
    "data_file_path = \"../../feathr_project/test/test_user_workspace/green_tripdata_2020-04_with_index.csv\"\n",
    "df_raw = nyc_taxi.get_spark_df(spark=spark, local_cache_path=data_file_path)\n",
    "df_raw.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_COL = \"lpep_dropoff_datetime\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get features from register server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:26:19.656 | INFO     | feathr.client:get_features_from_registry:1147 - Get anchor features from registry: \n",
      "2024-09-06 16:26:19.657 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_trip_distance\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"FLOAT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"trip_distance\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.658 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_trip_time_duration\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"FLOAT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"cast_float((to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime)) / 60)\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.658 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_is_long_trip_distance\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"BOOLEAN\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"trip_distance > 30.0\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.658 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_day_of_week\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"INT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"dayofweek(lpep_dropoff_datetime)\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.659 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_day_of_month\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"INT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"dayofmonth(lpep_dropoff_datetime)\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.660 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_hour_of_day\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"INT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"hour(lpep_dropoff_datetime)\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.660 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_location_avg_fare\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"FLOAT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"DOLocationID\",\n",
      "      \"keyColumnType\": \"INT\",\n",
      "      \"fullName\": \"nyc_taxi.location_id\",\n",
      "      \"description\": \"nyc_taxi.location_id\",\n",
      "      \"keyColumnAlias\": \"DOLocationID\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"defExpr\": \"fare_amount_cents\",\n",
      "    \"aggFunc\": \"AVG\",\n",
      "    \"window\": \"90d\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.660 | INFO     | feathr.client:get_features_from_registry:1153 - {\n",
      "  \"name\": \"f_location_max_fare\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"FLOAT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"DOLocationID\",\n",
      "      \"keyColumnType\": \"INT\",\n",
      "      \"fullName\": \"nyc_taxi.location_id\",\n",
      "      \"description\": \"nyc_taxi.location_id\",\n",
      "      \"keyColumnAlias\": \"DOLocationID\"\n",
      "    }\n",
      "  ],\n",
      "  \"transformation\": {\n",
      "    \"defExpr\": \"fare_amount_cents\",\n",
      "    \"aggFunc\": \"MAX\",\n",
      "    \"window\": \"90d\"\n",
      "  }\n",
      "}\n",
      "2024-09-06 16:26:19.661 | INFO     | feathr.client:get_features_from_registry:1155 - Get derived features from registry: \n",
      "2024-09-06 16:26:19.661 | INFO     | feathr.client:get_features_from_registry:1160 - {\n",
      "  \"name\": \"f_trip_speed\",\n",
      "  \"featureType\": {\n",
      "    \"type\": \"TENSOR\",\n",
      "    \"tensorCategory\": \"DENSE\",\n",
      "    \"dimensionType\": [],\n",
      "    \"valType\": \"FLOAT\"\n",
      "  },\n",
      "  \"key\": [\n",
      "    {\n",
      "      \"keyColumn\": \"NOT_NEEDED\",\n",
      "      \"keyColumnType\": \"UNSPECIFIED\",\n",
      "      \"fullName\": \"feathr.dummy_typedkey\",\n",
      "      \"description\": \"feathr.dummy_typedkey\",\n",
      "      \"keyColumnAlias\": \"NOT_NEEDED\"\n",
      "    }\n",
      "  ],\n",
      "  \"inputAnchorFeatures\": [\n",
      "    \"67a74383-ab2d-4621-894e-51a4c4f79770\",\n",
      "    \"bda71a00-67b9-49f2-8e35-51efc3f484e1\"\n",
      "  ],\n",
      "  \"inputDerivedFeatures\": [],\n",
      "  \"transformation\": {\n",
      "    \"transformExpr\": \"f_trip_distance / f_trip_time_duration\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "feature_dict = client.get_features_from_registry(project_name=PROJECT_NAME, return_keys=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_trip_distance',\n",
       " 'f_trip_time_duration',\n",
       " 'f_is_long_trip_distance',\n",
       " 'f_day_of_week',\n",
       " 'f_day_of_month',\n",
       " 'f_hour_of_day',\n",
       " 'f_location_avg_fare',\n",
       " 'f_location_max_fare',\n",
       " 'f_trip_speed']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[feat.name for feat in list(feature_dict[0].values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOT_NEEDED',\n",
       " 'NOT_NEEDED',\n",
       " 'NOT_NEEDED',\n",
       " 'NOT_NEEDED',\n",
       " 'NOT_NEEDED',\n",
       " 'NOT_NEEDED',\n",
       " 'DOLocationID',\n",
       " 'DOLocationID',\n",
       " 'NOT_NEEDED']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type_key.key_column for type_keys in list(feature_dict[1].values()) for type_key in type_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a226026-1c7b-48db-8f91-88d5c2ddf023",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Materialize Feature Values for Online Scoring\n",
    "\n",
    "While we computed feature values on-the-fly at request time via Feathr, we can pre-compute the feature values and materialize them to offline or online storages such as Redis.\n",
    "\n",
    "Note, only the features anchored to offline data source can be materialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 5, 1, 0, 47)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last date from the dataset\n",
    "backfill_timestamp = (\n",
    "    df_raw\n",
    "    .select(F.to_timestamp(F.col(TIMESTAMP_COL), TIMESTAMP_FORMAT).alias(TIMESTAMP_COL))\n",
    "    .agg({TIMESTAMP_COL: \"max\"})\n",
    "    .collect()[0][0]\n",
    ")\n",
    "backfill_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b924c66-8634-42fe-90f3-c844487d3f75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:26:20.423 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-09-06 16:26:20.424 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:20.424 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-06 16:26:20.425 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-09-06 16:26:20.425 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-09-06 16:26:20.425 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:296 - Spark log path is debug/nyc_taxi_feathr_feature_materialization_job20240906162620\n",
      "2024-09-06 16:26:20.427 | INFO     | feathr.spark_provider._localspark_submission:_init_args:271 - Spark job: nyc_taxi_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-09-06 16:26:20.432 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:151 - Detail job stdout and stderr are in debug/nyc_taxi_feathr_feature_materialization_job20240906162620/log.\n",
      "bash: /mnt/e/setup/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2024-09-06 16:26:20.433 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:161 - Local Spark job submit with pid: 110517.\n",
      "2024-09-06 16:26:20.435 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:171 - 1 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-09-06 16:26:20.435 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:172 - Please check auto generated spark command in debug/nyc_taxi_feathr_feature_materialization_job20240906162620/command.sh and detail logs in debug/nyc_taxi_feathr_feature_materialization_job20240906162620/log.\n",
      "bash: /mnt/e/setup/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "https://repository.mulesoft.org/nexus/content/repositories/public/ added as a remote repository with the name: repo-1\n",
      "https://linkedin.jfrog.io/artifactory/open-source/ added as a remote repository with the name: repo-2\n",
      "Ivy Default Cache set to: /home/cuong/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cuong/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.logging.log4j#log4j-core added as a dependency\n",
      "com.typesafe#config added as a dependency\n",
      "org.apache.hadoop#hadoop-mapreduce-client-core added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.avro#avro added as a dependency\n",
      "org.apache.xbean#xbean-asm6-shaded added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "com.jasonclawson#jackson-dataformat-hocon added as a dependency\n",
      "com.redislabs#spark-redis_2.12 added as a dependency\n",
      "org.apache.xbean#xbean-asm6-shaded added as a dependency\n",
      "com.google.protobuf#protobuf-java added as a dependency\n",
      "commons-io#commons-io added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3a566dfa-dbb0-4bae-ac2f-ade6e02b2f15;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.17.2 in central\n",
      "\tfound com.typesafe#config;1.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.2 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.2 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.13.0 in central\n",
      "\tfound javax.ws.rs#javax.ws.rs-api;2.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.2 in central\n",
      "\tfound com.squareup.okhttp#okhttp;2.7.5 in central\n",
      "\tfound com.squareup.okio#okio;1.6.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.13.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound com.google.inject.extensions#guice-servlet;4.0 in central\n",
      "\tfound com.google.inject#guice;4.0 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.sun.jersey.contribs#jersey-guice;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.13.0 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.13.0 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.13.0 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.30 in central\n",
      "\tfound org.jline#jline;3.9.0 in central\n",
      "\tfound io.netty#netty;3.10.6.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.avro#avro;1.8.2 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.7 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.1.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.0-4 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound com.jasonclawson#jackson-dataformat-hocon;1.1.0 in central\n",
      "\tfound com.redislabs#spark-redis_2.12;3.1.0 in central\n",
      "\tfound redis.clients#jedis;3.9.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 1539ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.5.0-4 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.inject#guice;4.0 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-servlet;4.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jasonclawson#jackson-dataformat-hocon;1.1.0 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.redislabs#spark-redis_2.12;3.1.0 from central in [default]\n",
      "\tcom.squareup.okhttp#okhttp;2.7.5 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.6.0 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.jersey.contribs#jersey-guice;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.7 from central in [default]\n",
      "\tcom.typesafe#config;1.3.4 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.netty#netty;3.10.6.Final from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.3 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#javax.ws.rs-api;2.1.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.8.2 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.1.0 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.17.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 from central in [default]\n",
      "\torg.jline#jline;3.9.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\tredis.clients#jedis;3.9.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 by [com.google.protobuf#protobuf-java;3.19.4] in [default]\n",
      "\torg.apache.avro#avro;1.7.7 by [org.apache.avro#avro;1.8.2] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.1.3 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]\n",
      "\torg.apache.commons#commons-compress;1.8.1 by [org.apache.commons#commons-compress;1.21] in [default]\n",
      "\torg.tukaani#xz;1.5 by [org.tukaani#xz;1.8] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 by [org.apache.kafka#kafka-clients;3.1.0] in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 by [org.apache.commons#commons-pool2;2.11.1] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.4.1.1 by [com.fasterxml.jackson.core#jackson-core;2.13.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.4.1 by [com.fasterxml.jackson.core#jackson-annotations;2.13.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.4.1.1 by [com.fasterxml.jackson.core#jackson-databind;2.13.0] in [default]\n",
      "\tcom.typesafe#config;1.2.1 by [com.typesafe#config;1.3.4] in [default]\n",
      "\tcommons-io#commons-io;2.6 by [commons-io#commons-io;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  146  |   0   |   0   |   14  ||  132  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3a566dfa-dbb0-4bae-ac2f-ade6e02b2f15\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 132 already retrieved (0kB/18ms)\n",
      "2024-09-06 16:26:53.467 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:198 - Pyspark job Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:26:54.470 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:234 - Spark job with pid 110517 finished in: 34 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    }
   ],
   "source": [
    "FEATURE_TABLE_NAME = \"nycTaxiDemoFeature\"\n",
    "\n",
    "# Time range to materialize\n",
    "backfill_time = BackfillTime(\n",
    "    start=backfill_timestamp,\n",
    "    end=backfill_timestamp,\n",
    "    step=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Destinations:\n",
    "# For online store,\n",
    "redis_sink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "\n",
    "# For offline store,\n",
    "# adls_sink = HdfsSink(output_path=)\n",
    "\n",
    "settings = MaterializationSettings(\n",
    "    name=FEATURE_TABLE_NAME + \".job\",  # job name\n",
    "    backfill_time=backfill_time,\n",
    "    sinks=[redis_sink],  # or adls_sink\n",
    "    feature_names=['f_location_avg_fare',\n",
    "                     'f_location_max_fare',],\n",
    ")\n",
    "\n",
    "client.materialize_features(\n",
    "    settings=settings,\n",
    "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\"},\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can retrieve features for online scoring as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'239': [1480.53271484375, 5707.0], '265': [4160.6171875, 10000.0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note, to get a single key, you may use client.get_online_features instead\n",
    "materialized_feature_values = client.multi_get_online_features(\n",
    "    feature_table=FEATURE_TABLE_NAME,\n",
    "    keys=[\"239\", \"265\"],\n",
    "    feature_names=['f_location_avg_fare',\n",
    "                     'f_location_max_fare',],\n",
    ")\n",
    "materialized_feature_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Unregister, delete cached files or do any other cleanups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark session if it is a local session.\n",
    "if is_jupyter():\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nyc_driver_demo",
   "notebookOrigID": 930353059183053,
   "widgets": {}
  },
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddb0e38f168d5afaa0b8ab4851ddd8c14364f1d087c15de6ff2ee5a559aec1f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
