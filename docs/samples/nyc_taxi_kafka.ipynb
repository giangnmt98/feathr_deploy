{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "384e5e16-7213-4186-9d04-09d03b155534",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feathr Quick Start Notebook\n",
    "\n",
    "1. run zookeeper-server\n",
    "\n",
    "    sudo /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties\n",
    "\n",
    "2. run kafka-server\n",
    "\n",
    "    sudo /usr/local/kafka/bin/kafka-server-start.sh  /usr/local/kafka/config/server.properties\n",
    "\n",
    "3. create topic nyc_driver_test\n",
    "\n",
    "    /usr/local/kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic nyc_driver_test\n",
    "\n",
    "4. check\n",
    "\n",
    "    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server=localhost:9092 --list\n",
    "    \n",
    "    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server=localhost:9092 --describe --topic nyc_driver_test\n",
    "    \n",
    "5. run python nyc_taxi_kafka_producer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Feathr and Necessary Dependancies\n",
    "\n",
    "Install feathr and necessary packages by running one of following commends if you haven't installed them already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80223a02-631c-40c8-91b3-a037249ffff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import feathr\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    # Feature data types\n",
    "    BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    # Feature data sources\n",
    "    INPUT_CONTEXT, HdfsSource,\n",
    "    # Feature aggregations\n",
    "    TypedKey, WindowAggTransformation,\n",
    "    # Feature types and anchor\n",
    "    DerivedFeature, Feature, FeatureAnchor,\n",
    "    # Materialization\n",
    "    BackfillTime, MaterializationSettings, RedisSink,\n",
    "    # Offline feature computation\n",
    "    FeatureQuery, ObservationSettings,\n",
    ")\n",
    "from feathr.datasets import nyc_taxi\n",
    "from feathr.spark_provider.feathr_configurations import SparkExecutionConfiguration\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from feathr.utils.platform import is_databricks, is_jupyter\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cd64e3a-376c-48e6-ba41-5197f3591d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "api_version: 1\n",
      "project_config:\n",
      "  project_name: nyc_taxi_kafka\n",
      "  \n",
      "spark_config:\n",
      "  # choice for spark runtime. Currently support: azure_synapse, databricks, local\n",
      "  spark_cluster: 'local'\n",
      "  spark_result_output_parts: '1'\n",
      "  local:\n",
      "    master: 'local[*]'\n",
      "    feathr_runtime_location: './vnpt_feathr-0.0.1.jar'\n",
      "\n",
      "online_store:\n",
      "  redis:\n",
      "    # Redis configs to access Redis cluster\n",
      "    host: '127.0.0.1'\n",
      "    port: 6379\n",
      "    ssl_enabled: False\n",
      "\n",
      "feature_registry:\n",
      "  # The API endpoint of the registry service\n",
      "  api_endpoint: \"http://127.0.0.1:8000/api/v1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"foobared\"  # default password for Redis\n",
    "import glob\n",
    "jar_name = glob.glob(\"./*.jar\")[0]\n",
    "\n",
    "# Make sure we get the Feathr jar name, assuming we just have one jar file.\n",
    "PROJECT_NAME = \"nyc_taxi_kafka\"\n",
    "yaml_config = f\"\"\"\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: {PROJECT_NAME}\n",
    "  \n",
    "spark_config:\n",
    "  # choice for spark runtime. Currently support: azure_synapse, databricks, local\n",
    "  spark_cluster: 'local'\n",
    "  spark_result_output_parts: '1'\n",
    "  local:\n",
    "    master: 'local[*]'\n",
    "    feathr_runtime_location: '{jar_name}'\n",
    "\n",
    "online_store:\n",
    "  redis:\n",
    "    # Redis configs to access Redis cluster\n",
    "    host: '127.0.0.1'\n",
    "    port: 6379\n",
    "    ssl_enabled: False\n",
    "\n",
    "feature_registry:\n",
    "  # The API endpoint of the registry service\n",
    "  api_endpoint: \"http://127.0.0.1:8000/api/v1\"\n",
    "\"\"\"\n",
    "feathr_workspace_folder = Path(f\"./{PROJECT_NAME}_feathr_config.yaml\")\n",
    "feathr_workspace_folder.parent.mkdir(exist_ok=True, parents=True)\n",
    "feathr_workspace_folder.write_text(yaml_config)\n",
    "print(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the configurations can be overwritten by environment variables with concatenation of `__` for different layers of the config file, same as how you may pass the keyword arguments to `generate_config` utility function.\n",
    "\n",
    "For example, `feathr_runtime_location` for databricks config can be overwritten by setting `spark_config__databricks__feathr_runtime_location` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fef7f2f-df19-4f53-90a5-ff7999ed983d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9713a2df-c7b2-4562-88b0-b7acce3cc43a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 14:02:55.579 | INFO     | feathr.utils._env_config_reader:get:62 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.580 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.581 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.581 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.582 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.582 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "No offline storage enabled.\n",
      "2024-08-08 14:02:55.583 | INFO     | feathr.utils._env_config_reader:get:62 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.584 | INFO     | feathr.utils._env_config_reader:get:62 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:02:55.584 | INFO     | feathr.client:__init__:208 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3b64bda-d42c-4a64-b976-0fb604cf38c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare the NYC taxi fare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_COL = \"lpep_dropoff_datetime\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feathr import (\n",
    "    AvroJsonSchema,\n",
    "    KafKaSource,\n",
    "    KafkaConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data schema\n",
    "schema = AvroJsonSchema(schemaStr=\"\"\"\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"DriverTrips\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"driver_id\", \"type\": \"long\"},\n",
    "        {\"name\": \"trips_today\", \"type\": \"int\"},\n",
    "        {\n",
    "        \"name\": \"datetime\",\n",
    "        \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\")\n",
    "stream_source = KafKaSource(name=\"kafkaStreamingSource\",\n",
    "                            kafkaConfig=KafkaConfig(brokers=[\"localhost:9092\"],\n",
    "                                                    topics=[\"nyc_driver_test\"],\n",
    "                                                    schema=schema)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a373ecbe-a040-4cd3-9d87-0d5f4c5ba553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "driver_id = TypedKey(key_column=\"driver_id\",\n",
    "                     key_column_type=ValueType.INT64,\n",
    "                     description=\"driver id\",\n",
    "                     full_name=\"nyc driver id\")\n",
    "\n",
    "kafkaAnchor = FeatureAnchor(name=\"kafkaAnchor\",\n",
    "                            source=stream_source,\n",
    "                            features=[Feature(name=\"f_modified_streaming_count\",\n",
    "                                              feature_type=INT32,\n",
    "                                              transform=\"trips_today + 1\",\n",
    "                                              key=driver_id),\n",
    "                                      Feature(name=\"f_modified_streaming_count2\",\n",
    "                                              feature_type=INT32,\n",
    "                                              transform=\"trips_today + randn() * cos(trips_today)\",\n",
    "                                              key=driver_id)]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[kafkaAnchor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTER_FEATURES = False\n",
    "if REGISTER_FEATURES:\n",
    "    try:\n",
    "        client.register_features()\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "    print(client.list_registered_features(project_name=PROJECT_NAME))\n",
    "    # You can get the actual features too by calling client.get_features_from_registry(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 14:14:54.325 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config KAFKA_SASL_JAAS_CONFIG is not found in the environment variable or the remote key value store.\n",
      "2024-08-08 14:14:54.326 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__url is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:14:54.327 | INFO     | feathr.utils._env_config_reader:get:62 - Config monitoring__database__sql__user is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-08-08 14:14:54.328 | WARNING  | feathr.utils._env_config_reader:get_from_env_or_akv:88 - Config MONITORING_DATABASE_SQL_PASSWORD is not found in the environment variable or the remote key value store.\n",
      "2024-08-08 14:14:54.328 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:78 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-08-08 14:14:54.328 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:292 - Spark log path is debug/nyc_taxi_kafka_feathr_feature_materialization_job20240808141454\n",
      "2024-08-08 14:14:54.329 | INFO     | feathr.spark_provider._localspark_submission:_init_args:267 - Spark job: nyc_taxi_kafka_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2024-08-08 14:14:54.339 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:147 - Detail job stdout and stderr are in debug/nyc_taxi_kafka_feathr_feature_materialization_job20240808141454/log.\n",
      "2024-08-08 14:14:54.341 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:157 - Local Spark job submit with pid: 15076.\n",
      "2024-08-08 14:14:54.342 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:167 - 4 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-08-08 14:14:54.343 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:168 - Please check auto generated spark command in debug/nyc_taxi_kafka_feathr_feature_materialization_job20240808141454/command.sh and detail logs in debug/nyc_taxi_kafka_feathr_feature_materialization_job20240808141454/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 14:15:26.378 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:233 - Spark job with pid 15076 finished in: 32 seconds                     with returncode 0\n"
     ]
    }
   ],
   "source": [
    "redisSink = RedisSink(table_name=\"kafkaSampleDemoFeature\", streaming=True, streamingTimeoutMs=10000)\n",
    "# The 'streamingTimeoutMs' parameter may need to be increased, such as 10min or even longer, to make sure some data sources from Kafka was captured.\n",
    "settings = MaterializationSettings(name=\"kafkaSampleDemo\",\n",
    "                                   sinks=[redisSink],\n",
    "                                   feature_names=['f_modified_streaming_count', 'f_modified_streaming_count2']\n",
    "                                   )\n",
    "client.materialize_features(settings, \n",
    "                            allow_materialize_non_agg_feature=True) # Will streaming for 10 seconds since streamingTimeoutMs is 10000\n",
    "client.wait_job_to_finish(timeout_sec=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9.53262992576499]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.get_online_features('kafkaSampleDemoFeature', '9' ,\n",
    "                                 ['f_modified_streaming_count', \n",
    "                                  'f_modified_streaming_count2'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [2, 0.9999112386259574], '2': [3, 2.3240336962830472]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get features for multiple feature keys\n",
    "res = client.multi_get_online_features('kafkaSampleDemoFeature', ['1', '2'], ['f_modified_streaming_count',\n",
    "                                                                              'f_modified_streaming_count2'\n",
    "                                                                             ])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can retrieve features for online scoring as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nyc_driver_demo",
   "notebookOrigID": 930353059183053,
   "widgets": {}
  },
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddb0e38f168d5afaa0b8ab4851ddd8c14364f1d087c15de6ff2ee5a559aec1f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
