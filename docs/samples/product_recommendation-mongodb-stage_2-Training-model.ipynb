{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5545a38-44a7-4aca-be6d-a66c51c75ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Product Recommendation with Feathr\n",
    "\n",
    "This notebook illustrates the use of Feathr Feature Store to create a model that predict users' rating for different products for a e-commerce website.\n",
    "\n",
    "### Model Problem Statement\n",
    "The e-commerce website has collected past user ratings for various products. The website also collected data about user and product, like user age, product category etc. Now we want to predict users' product rating for new product so that we can recommend the new product to users that give a high rating for those products.\n",
    "\n",
    "### Feature Creation Illustration\n",
    "In this example, our observation data has compound entity key where a record is uniquely identified by `user_id` and `product_id`. With that, we can think about three types of features:\n",
    "1. **User features** that are different for different users but are the same for different products. For example, user age is different for different users but it's product-agnostic.\n",
    "2. **Product features** that are different for different products but are the same for all the users.\n",
    "3. **User-to-product** features that are different for different users AND different products. For example, a feature to represent if the user has bought this product before or not.\n",
    "\n",
    "In this example, we will focus on the first two types of features. After we train a model based on those features, we predict the product ratings that users will give for the products.\n",
    "\n",
    "The feature creation flow is as below:\n",
    "![Feature Flow](https://github.com/feathr-ai/feathr/blob/main/docs/images/product_recommendation_advanced.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config Feathr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f3135eb-15c5-4f46-90ff-881a21cc59df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "# from interpret.provider import InlineProvider\n",
    "# from interpret import set_visualize_provider\n",
    "\n",
    "# set_visualize_provider(InlineProvider())\n",
    "\n",
    "import feathr\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    Feature, DerivedFeature, FeatureAnchor,\n",
    "    BackfillTime, MaterializationSettings,\n",
    "    FeatureQuery, ObservationSettings,\n",
    "    RedisSink, MongoDbSink,\n",
    "    INPUT_CONTEXT, JdbcSource,\n",
    "    WindowAggTransformation,\n",
    "    TypedKey,\n",
    ")\n",
    "from feathr.datasets.constants import (\n",
    "    PRODUCT_RECOMMENDATION_USER_OBSERVATION_URL,\n",
    "    PRODUCT_RECOMMENDATION_USER_PROFILE_URL,\n",
    "    PRODUCT_RECOMMENDATION_USER_PURCHASE_HISTORY_URL,\n",
    "    PRODUCT_RECOMMENDATION_PRODUCT_DETAIL_URL,\n",
    ")\n",
    "from feathr.datasets.utils import maybe_download\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "os.environ['REDIS_PASSWORD'] = \"foobared\"\n",
    "PROJECT_NAME = \"product_recommendation_from_mongodb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "794492ed-66b0-4787-adc6-3f234c4739a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize Feathr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c748f9d-210b-4c1d-a414-b30328d5e219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 12:54:29.566 | INFO     | feathr.utils._env_config_reader:get:62 - Config secrets__azure_key_vault__name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.568 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__s3__s3_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.568 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__adls__adls_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.569 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__wasb__wasb_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.569 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__jdbc__jdbc_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.570 | INFO     | feathr.utils._env_config_reader:get:62 - Config offline_store__snowflake__snowflake_enabled is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "No offline storage enabled.\n",
      "2024-09-11 12:54:29.571 | INFO     | feathr.utils._env_config_reader:get:62 - Config spark_config__local__workspace is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.571 | INFO     | feathr.utils._env_config_reader:get:62 - Config feature_registry__purview__purview_name is not found in the environment variable, configuration file, or the remote key value store. Returning the default value: None.\n",
      "2024-09-11 12:54:29.571 | INFO     | feathr.client:__init__:208 - Feathr client 1.0.0 initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "feathr_workspace_folder = Path(f\"./{PROJECT_NAME}_feathr_config.yaml\")\n",
    "client = FeathrClient(str(feathr_workspace_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "46b45998-d933-4417-b152-7db091c0d5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Define Sharable Features using Feathr API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bdc5a2e1-ccd4-4d61-9168-b0e4f571587b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What's a feature in Feathr\n",
    "A feature is an individual measurable property or characteristic of a phenomenon which is sometimes time-sensitive.\n",
    "\n",
    "In Feathr, a feature is defined by the following characteristics:\n",
    "* The typed key (a.k.a. entity id): identifies the subject of feature, e.g. a user id of 123, a product id of SKU234456.\n",
    "* The feature name: the unique identifier of the feature, e.g. user_age, total_spending_in_30_days.\n",
    "* The feature value: the actual value of that aspect at a particular time, e.g. the feature value of the person's age is 30 at year 2022.\n",
    "* The timestamp: this indicates when the event happened. For example, the user purchased certain product on a certain timestamp. This is usually used for point-in-time join.\n",
    "\n",
    "You can feel that this is defined from a feature consumer (a person who wants to use a feature) perspective. It only tells us what a feature is like. In later sections, you can see how a feature consumer can access the features in a very simple way.\n",
    "\n",
    "To define how to produce the feature, we need to specify:\n",
    "* Feature source: what source data that this feature is based on\n",
    "* Transformation: what transformation is used to transform the source data into feature. Transformation can be optional when you just want to take a column out from the source data.\n",
    "\n",
    "(For more details on feature definition, please refer to the [Feathr Feature Definition Guide](https://feathr-ai.github.io/feathr/concepts/feature-definition.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30e2c57d-6487-4d72-bd78-80d17325f1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note: in some cases, such as features defined on top of request data, may have no entity key or timestamp.\n",
    "It is merely a function/transformation executing against request data at runtime.\n",
    "For example, the day of week of the request, which is calculated by converting the request UNIX timestamp.\n",
    "(We won't cover this in the tutorial.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64fc4ef8-ccde-4724-8eff-1263c08de39f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define Sources Section with UDFs\n",
    "\n",
    "A feature is called an anchored feature when the feature is directly extracted from the source data, rather than computed on top of other features. The latter case is called derived feature.\n",
    "\n",
    "A [feature source](https://feathr.readthedocs.io/en/latest/#feathr.Source) is needed for anchored features that describes the raw data in which the feature values are computed from. See the python documentation to get the details on each input column.\n",
    "\n",
    "See [the python API documentation](https://feathr.readthedocs.io/en/latest/#feathr.HdfsSource) to get the details of each input fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78e240b4-dcab-499f-b6ed-72a14bfab968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define window aggregation features\n",
    "\n",
    "[Window aggregation](https://en.wikipedia.org/wiki/Window_function_%28SQL%29) helps us to create more powerful features by compressing large amount of information. For example, we can compute *average purchase amount over the last 90 days* from the purchase history to capture user's recent consumption trend.\n",
    "\n",
    "To create window aggregation features, we define `WindowAggTransformation` with following arguments:\n",
    "1. `agg_expr`: the field/column you want to aggregate. It can be an ANSI SQL expression, e.g. `cast_float(purchase_amount)` to cast `str` type values to `float`.\n",
    "2. `agg_func`: the aggregation function, e.g. `AVG`. See below table for the full list of supported functions.\n",
    "3. `window`: the aggregation window size, e.g. `90d` to aggregate over the 90 days.\n",
    "\n",
    "| Aggregation Type | Input Type | Description |\n",
    "| --- | --- | --- |\n",
    "| `SUM`, `COUNT`, `MAX`, `MIN`, `AVG` | Numeric | Applies the the numerical operation on the numeric inputs. |\n",
    "| `MAX_POOLING`, `MIN_POOLING`, `AVG_POOLING`\t| Numeric Vector | Applies the max/min/avg operation on a per entry basis for a given a collection of numbers. |\n",
    "| `LATEST` | Any | Returns the latest not-null values from within the defined time window. |\n",
    "\n",
    "After you have defined features and sources, bring them together to build an anchor:\n",
    "\n",
    "> Note that if the features comes directly from the observation data, the `source` argument should be `INPUT_CONTEXT` to indicate the source of the anchor is the observation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features from Register Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'feature_user_tax_rate',\n",
       "  'id': '8ff6b2d3-a56d-46b6-b7c3-092f64e3495b',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__anchored_features__feature_user_tax_rate'},\n",
       " {'name': 'feature_user_purchasing_power',\n",
       "  'id': '5ca3e72b-036e-47e1-826c-ed5f07f24ceb',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__feature_user_purchasing_power'},\n",
       " {'name': 'feature_user_gift_card_balance',\n",
       "  'id': '8c316e41-f6ae-4dad-8f4d-f7935bb2838e',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__anchored_features__feature_user_gift_card_balance'},\n",
       " {'name': 'feature_user_has_valid_credit_card',\n",
       "  'id': '4b525ff5-ea5e-4896-8cfc-713b9d010109',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__anchored_features__feature_user_has_valid_credit_card'},\n",
       " {'name': 'feature_user_avg_purchase_for_90days',\n",
       "  'id': 'ffb417d0-72df-4756-9d34-839e3d93b5cd',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__aggregationFeatures__feature_user_avg_purchase_for_90days'},\n",
       " {'name': 'feature_product_quantity',\n",
       "  'id': '74726e72-9a6c-4409-8f58-390c57db09c9',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__product_anchored_features__feature_product_quantity'},\n",
       " {'name': 'feature_user_age',\n",
       "  'id': '1db17e79-c4d9-428f-afe0-98ddc0a16e8e',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__anchored_features__feature_user_age'},\n",
       " {'name': 'feature_product_price',\n",
       "  'id': '601cc54a-34f9-4f90-b998-572bef68d327',\n",
       "  'qualifiedName': 'product_recommendation_from_mongodb__product_anchored_features__feature_product_price'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_registered_features(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = client.get_features_from_registry(project_name=PROJECT_NAME, \n",
    "                                                 return_keys=True, \n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can list all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_user_avg_purchase_for_90days',\n",
       " 'feature_product_quantity',\n",
       " 'feature_product_price',\n",
       " 'feature_user_gift_card_balance',\n",
       " 'feature_user_has_valid_credit_card',\n",
       " 'feature_user_age',\n",
       " 'feature_user_tax_rate',\n",
       " 'feature_user_purchasing_power']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[feat.name for feat in list(feature_dict[0].values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can list all type_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'product_id',\n",
       " 'product_id',\n",
       " 'user_id',\n",
       " 'user_id',\n",
       " 'user_id',\n",
       " 'user_id',\n",
       " 'user_id']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type_key.key_column for type_keys in list(feature_dict[1].values()) for type_key in type_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b2877d0-2ab8-4c07-99d4-effc7336ee8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Create Training Data using Point-in-Time Correct Feature join\n",
    "\n",
    "To create a training dataset using Feathr, we need to provide a **feature join settings** to specify what features and how these features should be joined to the observation data.\n",
    "\n",
    "Also note that since a `FeatureQuery` accepts features of the same join key, we define two query objects, one for `user_id` key and the other one for `product_id` and pass them together to compute offline features.\n",
    "\n",
    "To learn more on this topic, please refer to [Point-in-time Correctness document](https://feathr-ai.github.io/feathr/concepts/point-in-time-join.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_user_id = feature_dict[1][\"feature_user_avg_purchase_for_90days\"][0]\n",
    "key_product_id = feature_dict[1][\"feature_product_quantity\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_query = FeatureQuery(\n",
    "    feature_list=[feat.name for feat in list(feature_dict[0].values()) if \"user\" in feat.name], #[feat.name for feat in features + agg_features + derived_features],\n",
    "    key=key_user_id, #user_id\n",
    ")\n",
    "\n",
    "product_feature_query = FeatureQuery(\n",
    "    feature_list= [feat.name for feat in list(feature_dict[0].values()) if \"product\" in feat.name] ,#[feat.name for feat in product_features],\n",
    "    key=key_product_id, #product_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USERPROFILEDATA_USER'] = \"root\"\n",
    "os.environ['USERPROFILEDATA_PASSWORD'] = \"example\"\n",
    "\n",
    "\n",
    "os.environ['PRODUCTPROFILEDATA_USER'] = \"root\"\n",
    "os.environ['PRODUCTPROFILEDATA_PASSWORD'] = \"example\"\n",
    "\n",
    "\n",
    "os.environ['PURCHASE_HISTORY_DATA_USER'] = \"root\"\n",
    "os.environ['PURCHASE_HISTORY_DATA_PASSWORD'] = \"example\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_observation_collection = \"user_observation\"\n",
    "uri = f\"mongodb://127.0.0.1:27017/feastore.{user_observation_collection}?authSource=admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_uri = f\"mongodb://127.0.0.1:27017/feastore.recommendation_product_trainingdata?authSource=admin\"\n",
    "mongodb_sink = MongoDbSink(\"recommendation_product_trainingdata\", save_uri)\n",
    "\n",
    "os.environ['RECOMMENDATION_PRODUCT_TRAININGDATA_USER'] = \"root\"\n",
    "os.environ['RECOMMENDATION_PRODUCT_TRAININGDATA_PASSWORD'] = \"example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30302a53-561f-4b85-ba25-8de9fc843c63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:feathr.client:Feathr is unable to read the Observation data from mongodb://127.0.0.1:27017/feastore.user_observation?authSource=admin due to permission issue or invalid path. Please either grant the permission or supply the observation column names in the filed: observation_column_names.\n",
      "2024-09-11 12:54:30.816 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:79 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2024-09-11 12:54:30.816 | INFO     | feathr.spark_provider._localspark_submission:_get_debug_file_name:299 - Spark log path is debug/product_recommendation_from_mongodb_feathr_feature_join_job20240911125430\n",
      "2024-09-11 12:54:30.817 | INFO     | feathr.spark_provider._localspark_submission:_init_args:274 - Spark job: product_recommendation_from_mongodb_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2024-09-11 12:54:30.823 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:154 - Detail job stdout and stderr are in debug/product_recommendation_from_mongodb_feathr_feature_join_job20240911125430/log.\n",
      "bash: /mnt/e/setup/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2024-09-11 12:54:30.826 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:164 - Local Spark job submit with pid: 1976528.\n",
      "2024-09-11 12:54:30.827 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:174 - 1 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2024-09-11 12:54:30.827 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:175 - Please check auto generated spark command in debug/product_recommendation_from_mongodb_feathr_feature_join_job20240911125430/command.sh and detail logs in debug/product_recommendation_from_mongodb_feathr_feature_join_job20240911125430/log.\n",
      "bash: /mnt/e/setup/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "https://repository.mulesoft.org/nexus/content/repositories/public/ added as a remote repository with the name: repo-1\n",
      "https://linkedin.jfrog.io/artifactory/open-source/ added as a remote repository with the name: repo-2\n",
      "Ivy Default Cache set to: /home/cuong/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cuong/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.apache.logging.log4j#log4j-core added as a dependency\n",
      "com.typesafe#config added as a dependency\n",
      "org.apache.hadoop#hadoop-mapreduce-client-core added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.avro#avro added as a dependency\n",
      "org.apache.xbean#xbean-asm6-shaded added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "com.jasonclawson#jackson-dataformat-hocon added as a dependency\n",
      "com.redislabs#spark-redis_2.12 added as a dependency\n",
      "org.apache.xbean#xbean-asm6-shaded added as a dependency\n",
      "com.google.protobuf#protobuf-java added as a dependency\n",
      "commons-io#commons-io added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-939ec0ee-c1f0-4ada-84e1-b6aa7535c52c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.17.2 in central\n",
      "\tfound com.typesafe#config;1.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.2 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.2 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.13.0 in central\n",
      "\tfound javax.ws.rs#javax.ws.rs-api;2.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.2 in central\n",
      "\tfound com.squareup.okhttp#okhttp;2.7.5 in central\n",
      "\tfound com.squareup.okio#okio;1.6.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.13.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in spark-list\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound com.google.inject.extensions#guice-servlet;4.0 in central\n",
      "\tfound com.google.inject#guice;4.0 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in spark-list\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in spark-list\n",
      "\tfound com.sun.jersey.contribs#jersey-guice;1.19 in spark-list\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.13.0 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.13.0 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.13.0 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.30 in central\n",
      "\tfound org.jline#jline;3.9.0 in central\n",
      "\tfound io.netty#netty;3.10.6.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.avro#avro;1.8.2 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.7 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.1.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.0-4 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound com.jasonclawson#jackson-dataformat-hocon;1.1.0 in central\n",
      "\tfound com.redislabs#spark-redis_2.12;3.1.0 in central\n",
      "\tfound redis.clients#jedis;3.9.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 8044ms :: artifacts dl 40ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.5.0-4 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.inject#guice;4.0 from central in [default]\n",
      "\tcom.google.inject.extensions#guice-servlet;4.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jasonclawson#jackson-dataformat-hocon;1.1.0 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.redislabs#spark-redis_2.12;3.1.0 from central in [default]\n",
      "\tcom.squareup.okhttp#okhttp;2.7.5 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.6.0 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.jersey.contribs#jersey-guice;1.19 from spark-list in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.7 from central in [default]\n",
      "\tcom.typesafe#config;1.3.4 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.netty#netty;3.10.6.Final from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.3 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#javax.ws.rs-api;2.1.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.8.2 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.1.0 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from spark-list in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.17.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from spark-list in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from spark-list in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 from central in [default]\n",
      "\torg.jline#jline;3.9.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\tredis.clients#jedis;3.9.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 by [com.google.protobuf#protobuf-java;3.19.4] in [default]\n",
      "\torg.apache.avro#avro;1.7.7 by [org.apache.avro#avro;1.8.2] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.1.3 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]\n",
      "\torg.apache.commons#commons-compress;1.8.1 by [org.apache.commons#commons-compress;1.21] in [default]\n",
      "\torg.tukaani#xz;1.5 by [org.tukaani#xz;1.8] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 by [org.apache.kafka#kafka-clients;3.1.0] in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 by [org.apache.commons#commons-pool2;2.11.1] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.4.1.1 by [com.fasterxml.jackson.core#jackson-core;2.13.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.4.1 by [com.fasterxml.jackson.core#jackson-annotations;2.13.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.4.1.1 by [com.fasterxml.jackson.core#jackson-databind;2.13.0] in [default]\n",
      "\tcom.typesafe#config;1.2.1 by [com.typesafe#config;1.3.4] in [default]\n",
      "\tcommons-io#commons-io;2.6 by [commons-io#commons-io;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  151  |   1   |   0   |   14  ||  137  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-939ec0ee-c1f0-4ada-84e1-b6aa7535c52c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 137 already retrieved (0kB/15ms)\n",
      "2024-09-11 12:55:03.865 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:201 - Pyspark job Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 12:55:04.870 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:237 - Spark job with pid 1976528 finished in: 34 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">"
     ]
    }
   ],
   "source": [
    "settings = ObservationSettings(\n",
    "    observation_path=uri,\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    "    timestamp_format=\"yyyy-MM-dd\",\n",
    ")\n",
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=[user_feature_query, product_feature_query],\n",
    "    output_path=mongodb_sink, #f\"{PROJECT_NAME}/trainingdata\",\n",
    "    execution_configurations={\"spark.feathr.outputFormat\": \"parquet\",\n",
    "                              \"spark.feathr.mongodb.outputExtendedJson\": \"false\",\n",
    "                              \"spark.feathr.mongodb.convertJson\": \"any\",\n",
    "                              \"spark.feathr.mongodb.user\": \"root\",\n",
    "                              \"spark.feathr.mongodb.password\": \"example\",\n",
    "                              \"spark.feathr.mongodb.aggregation.pipeline\": \"'[{$match:{}}]'\",\n",
    "                              \"spark.feathr.mongodb.aggregation.allowDiskUse\": \"true\", # it's a demo when we want to use query\n",
    "                              'spark.jars.plus.packages':\"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\"\n",
    "                             }\n",
    ")\n",
    "client.wait_job_to_finish(timeout_sec=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc7b6276-70c1-494f-83ca-53d442e3198a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use the helper function `get_result_df` to download the result and view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the notebook is runnong on Jupyter, start a spark session:\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from feathr.utils.platform import is_databricks, is_jupyter\n",
    "if is_jupyter():\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"feathr\")\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mongodb://root:example@127.0.0.1:27017/feastore.recommendation_product_trainingdata?authSource=admin'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = save_uri.replace(\"mongodb://\", f\"mongodb://{os.environ['RECOMMENDATION_PRODUCT_TRAININGDATA_USER']}:{os.environ['RECOMMENDATION_PRODUCT_TRAININGDATA_PASSWORD']}@\")\n",
    "uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = spark.read.format(\"mongodb\").option(\"spark.mongodb.read.connection.uri\", uri).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "res_df = dataFrame.withColumn(\"event_timestamp\", \n",
    "                              date_format(\"event_timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "                             ).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "497d6a3b-94e2-4087-94b1-0a5d7baf3ab3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train a machine learning model\n",
    "After getting all the features, let's train a machine learning model with the converted feature by Feathr. Here, we use **EBM (Explainable Boosting Machine)** regressor from [InterpretML](https://github.com/interpretml/interpret) package to visualize the modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9bd661ae-430e-449b-9a62-9155828de099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/140369341762240/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/140369341762240/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interpret import show\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fill None values with 0\n",
    "final_df = (\n",
    "    res_df\n",
    "    .drop([\"event_timestamp\"], axis=1, errors=\"ignore\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_df.drop([\"Product_rating\"], axis=1),\n",
    "    final_df[\"Product_rating\"].astype(\"float64\"),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "ebm = ExplainableBoostingRegressor()\n",
    "ebm.fit(X_train, y_train)\n",
    "\n",
    "# show(ebm_global) # Will run on 127.0.0.1/localhost at port 7080\n",
    "# Note, currently InterpretML's visualization dashboard doesn't work w/ VSCODE notebook viewer\n",
    "# https://github.com/interpretml/interpret/issues/317\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 1.703444960993401\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = ebm.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test.values.flatten(), y_pred))\n",
    "\n",
    "print(f\"Root mean squared error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'feature_product_price', 'feature_product_quantity',\n",
       "       'feature_user_age', 'feature_user_avg_purchase_for_90days',\n",
       "       'feature_user_gift_card_balance', 'feature_user_has_valid_credit_card',\n",
       "       'feature_user_purchasing_power', 'feature_user_tax_rate', 'product_id',\n",
       "       'user_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "product_recommendation_demo_advanced",
   "notebookOrigID": 411375353096492,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "197b83f153f250c1ab26c6f20b4199af9ff4d61ee3c56eceb059d2c848b8ae73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
